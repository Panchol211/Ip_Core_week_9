{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IP_weeK9_Core.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVWajjBC3LqjIxkrGBPuDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Panchol211/Ip_Core_week_9/blob/main/IP_weeK9_Core.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIvFQR6HgmHN"
      },
      "source": [
        "\n",
        "##**K-NEAREST NEIGHBOR AND NAIVE BAYERS MODELS**##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-o0aE7Ugqod"
      },
      "source": [
        "##Problem Statement ##\n",
        "\n",
        "Implementation of K-nearest neighbor (kNN) classifier  and a Naive Bayes classifier to see which model performs better and under which conditions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4MDxEEjhK47"
      },
      "source": [
        "##Understanding the Business Context##\n",
        "\n",
        "\n",
        "Working on a titanic dataset and a spambase dataset.\n",
        "\n",
        "From titanic dataset we want to predict the survival rate of the people who where involved in that Titanic accident \n",
        "\n",
        "For the spambase dataset, we want to predict who will can the message be classifer as a spam or a genuine message\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfRWByLIiBgs"
      },
      "source": [
        "##Defining metrices of success##\n",
        "\n",
        "I would developed a model that can attain an accuracy of over 80% and challenge my findings to see how best i could have improved my model for a better prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELf_wgr3isjM"
      },
      "source": [
        "##Expreimental Design##\n",
        "\n",
        "Titanic Data set and SpamBase dataset be used\n",
        "\n",
        "Perform exploratory on the data and feature enginering \n",
        "\n",
        "Build the model and compare their performance \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZePxVOPjc2U"
      },
      "source": [
        "**Loading libraries to be used to analysis these datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB9zkh0r4p_k"
      },
      "source": [
        "# Imports\n",
        "\n",
        "# pandas\n",
        "import pandas as pd\n",
        "# numpy\n",
        "import numpy as np\n",
        "# pandas\n",
        "import pandas_profiling as pp\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import csv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhStoxnC5FOp"
      },
      "source": [
        "# Loading the spam dataset\n",
        "spam = pd.read_csv(\"spambase.data\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "autn0svl59BE",
        "outputId": "3240f0c8-fbd3-468b-a602-c56855a4276d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "# Viewing the top of the dataset\n",
        "spam.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0.64</th>\n",
              "      <th>0.64.1</th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.32</th>\n",
              "      <th>0.2</th>\n",
              "      <th>0.3</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.64.2</th>\n",
              "      <th>0.8</th>\n",
              "      <th>0.9</th>\n",
              "      <th>0.10</th>\n",
              "      <th>0.32.1</th>\n",
              "      <th>0.11</th>\n",
              "      <th>1.29</th>\n",
              "      <th>1.93</th>\n",
              "      <th>0.12</th>\n",
              "      <th>0.96</th>\n",
              "      <th>0.13</th>\n",
              "      <th>0.14</th>\n",
              "      <th>0.15</th>\n",
              "      <th>0.16</th>\n",
              "      <th>0.17</th>\n",
              "      <th>0.18</th>\n",
              "      <th>0.19</th>\n",
              "      <th>0.20</th>\n",
              "      <th>0.21</th>\n",
              "      <th>0.22</th>\n",
              "      <th>0.23</th>\n",
              "      <th>0.24</th>\n",
              "      <th>0.25</th>\n",
              "      <th>0.26</th>\n",
              "      <th>0.27</th>\n",
              "      <th>0.28</th>\n",
              "      <th>0.29</th>\n",
              "      <th>0.30</th>\n",
              "      <th>0.31</th>\n",
              "      <th>0.32.2</th>\n",
              "      <th>0.33</th>\n",
              "      <th>0.34</th>\n",
              "      <th>0.35</th>\n",
              "      <th>0.36</th>\n",
              "      <th>0.37</th>\n",
              "      <th>0.38</th>\n",
              "      <th>0.39</th>\n",
              "      <th>0.40</th>\n",
              "      <th>0.41</th>\n",
              "      <th>0.42</th>\n",
              "      <th>0.778</th>\n",
              "      <th>0.43</th>\n",
              "      <th>0.44</th>\n",
              "      <th>3.756</th>\n",
              "      <th>61</th>\n",
              "      <th>278</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>15</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0  0.64  0.64.1  0.1  0.32   0.2  ...   0.43   0.44  3.756   61   278  1\n",
              "0  0.21  0.28    0.50  0.0  0.14  0.28  ...  0.180  0.048  5.114  101  1028  1\n",
              "1  0.06  0.00    0.71  0.0  1.23  0.19  ...  0.184  0.010  9.821  485  2259  1\n",
              "2  0.00  0.00    0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191  1\n",
              "3  0.00  0.00    0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191  1\n",
              "4  0.00  0.00    0.00  0.0  1.85  0.00  ...  0.000  0.000  3.000   15    54  1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJCnpIwm6ATH",
        "outputId": "a678b81b-1cd8-44c5-a367-76f901ea240c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "# Viewing the bottom of the datatset\n",
        "spam.tail()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0.64</th>\n",
              "      <th>0.64.1</th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.32</th>\n",
              "      <th>0.2</th>\n",
              "      <th>0.3</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.64.2</th>\n",
              "      <th>0.8</th>\n",
              "      <th>0.9</th>\n",
              "      <th>0.10</th>\n",
              "      <th>0.32.1</th>\n",
              "      <th>0.11</th>\n",
              "      <th>1.29</th>\n",
              "      <th>1.93</th>\n",
              "      <th>0.12</th>\n",
              "      <th>0.96</th>\n",
              "      <th>0.13</th>\n",
              "      <th>0.14</th>\n",
              "      <th>0.15</th>\n",
              "      <th>0.16</th>\n",
              "      <th>0.17</th>\n",
              "      <th>0.18</th>\n",
              "      <th>0.19</th>\n",
              "      <th>0.20</th>\n",
              "      <th>0.21</th>\n",
              "      <th>0.22</th>\n",
              "      <th>0.23</th>\n",
              "      <th>0.24</th>\n",
              "      <th>0.25</th>\n",
              "      <th>0.26</th>\n",
              "      <th>0.27</th>\n",
              "      <th>0.28</th>\n",
              "      <th>0.29</th>\n",
              "      <th>0.30</th>\n",
              "      <th>0.31</th>\n",
              "      <th>0.32.2</th>\n",
              "      <th>0.33</th>\n",
              "      <th>0.34</th>\n",
              "      <th>0.35</th>\n",
              "      <th>0.36</th>\n",
              "      <th>0.37</th>\n",
              "      <th>0.38</th>\n",
              "      <th>0.39</th>\n",
              "      <th>0.40</th>\n",
              "      <th>0.41</th>\n",
              "      <th>0.42</th>\n",
              "      <th>0.778</th>\n",
              "      <th>0.43</th>\n",
              "      <th>0.44</th>\n",
              "      <th>3.756</th>\n",
              "      <th>61</th>\n",
              "      <th>278</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4595</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.142</td>\n",
              "      <td>3</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4596</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.555</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4597</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.404</td>\n",
              "      <td>6</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4598</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.147</td>\n",
              "      <td>5</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4599</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.97</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.250</td>\n",
              "      <td>5</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0  0.64  0.64.1  0.1  0.32   0.2  ...  0.43  0.44  3.756  61  278  1\n",
              "4595  0.31   0.0    0.62  0.0  0.00  0.31  ...   0.0   0.0  1.142   3   88  0\n",
              "4596  0.00   0.0    0.00  0.0  0.00  0.00  ...   0.0   0.0  1.555   4   14  0\n",
              "4597  0.30   0.0    0.30  0.0  0.00  0.00  ...   0.0   0.0  1.404   6  118  0\n",
              "4598  0.96   0.0    0.00  0.0  0.32  0.00  ...   0.0   0.0  1.147   5   78  0\n",
              "4599  0.00   0.0    0.65  0.0  0.00  0.00  ...   0.0   0.0  1.250   5   40  0\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQtmHY6U6HUH",
        "outputId": "f6c74ce5-d514-4f3e-8edd-9d131d3a07ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Checking the size of the dataset\n",
        "spam.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4600, 58)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMoVang46Ljo",
        "outputId": "125e28d0-8ef2-4f6e-d189-d2bfddc4082f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Checking the size of the set\n",
        "spam.size"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "266800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCoLfWCU6PWE",
        "outputId": "fdf7a12d-7b19-44cd-91ab-b5a12fc2d119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Information about the datatset\n",
        "spam.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4600 entries, 0 to 4599\n",
            "Data columns (total 58 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       4600 non-null   float64\n",
            " 1   0.64    4600 non-null   float64\n",
            " 2   0.64.1  4600 non-null   float64\n",
            " 3   0.1     4600 non-null   float64\n",
            " 4   0.32    4600 non-null   float64\n",
            " 5   0.2     4600 non-null   float64\n",
            " 6   0.3     4600 non-null   float64\n",
            " 7   0.4     4600 non-null   float64\n",
            " 8   0.5     4600 non-null   float64\n",
            " 9   0.6     4600 non-null   float64\n",
            " 10  0.7     4600 non-null   float64\n",
            " 11  0.64.2  4600 non-null   float64\n",
            " 12  0.8     4600 non-null   float64\n",
            " 13  0.9     4600 non-null   float64\n",
            " 14  0.10    4600 non-null   float64\n",
            " 15  0.32.1  4600 non-null   float64\n",
            " 16  0.11    4600 non-null   float64\n",
            " 17  1.29    4600 non-null   float64\n",
            " 18  1.93    4600 non-null   float64\n",
            " 19  0.12    4600 non-null   float64\n",
            " 20  0.96    4600 non-null   float64\n",
            " 21  0.13    4600 non-null   float64\n",
            " 22  0.14    4600 non-null   float64\n",
            " 23  0.15    4600 non-null   float64\n",
            " 24  0.16    4600 non-null   float64\n",
            " 25  0.17    4600 non-null   float64\n",
            " 26  0.18    4600 non-null   float64\n",
            " 27  0.19    4600 non-null   float64\n",
            " 28  0.20    4600 non-null   float64\n",
            " 29  0.21    4600 non-null   float64\n",
            " 30  0.22    4600 non-null   float64\n",
            " 31  0.23    4600 non-null   float64\n",
            " 32  0.24    4600 non-null   float64\n",
            " 33  0.25    4600 non-null   float64\n",
            " 34  0.26    4600 non-null   float64\n",
            " 35  0.27    4600 non-null   float64\n",
            " 36  0.28    4600 non-null   float64\n",
            " 37  0.29    4600 non-null   float64\n",
            " 38  0.30    4600 non-null   float64\n",
            " 39  0.31    4600 non-null   float64\n",
            " 40  0.32.2  4600 non-null   float64\n",
            " 41  0.33    4600 non-null   float64\n",
            " 42  0.34    4600 non-null   float64\n",
            " 43  0.35    4600 non-null   float64\n",
            " 44  0.36    4600 non-null   float64\n",
            " 45  0.37    4600 non-null   float64\n",
            " 46  0.38    4600 non-null   float64\n",
            " 47  0.39    4600 non-null   float64\n",
            " 48  0.40    4600 non-null   float64\n",
            " 49  0.41    4600 non-null   float64\n",
            " 50  0.42    4600 non-null   float64\n",
            " 51  0.778   4600 non-null   float64\n",
            " 52  0.43    4600 non-null   float64\n",
            " 53  0.44    4600 non-null   float64\n",
            " 54  3.756   4600 non-null   float64\n",
            " 55  61      4600 non-null   int64  \n",
            " 56  278     4600 non-null   int64  \n",
            " 57  1       4600 non-null   int64  \n",
            "dtypes: float64(55), int64(3)\n",
            "memory usage: 2.0 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA7zd2sx6TT0",
        "outputId": "ce65b5e1-05f0-4cd5-ba59-9a5549aa0e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "# Describing the data\n",
        "spam.describe()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0.64</th>\n",
              "      <th>0.64.1</th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.32</th>\n",
              "      <th>0.2</th>\n",
              "      <th>0.3</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.64.2</th>\n",
              "      <th>0.8</th>\n",
              "      <th>0.9</th>\n",
              "      <th>0.10</th>\n",
              "      <th>0.32.1</th>\n",
              "      <th>0.11</th>\n",
              "      <th>1.29</th>\n",
              "      <th>1.93</th>\n",
              "      <th>0.12</th>\n",
              "      <th>0.96</th>\n",
              "      <th>0.13</th>\n",
              "      <th>0.14</th>\n",
              "      <th>0.15</th>\n",
              "      <th>0.16</th>\n",
              "      <th>0.17</th>\n",
              "      <th>0.18</th>\n",
              "      <th>0.19</th>\n",
              "      <th>0.20</th>\n",
              "      <th>0.21</th>\n",
              "      <th>0.22</th>\n",
              "      <th>0.23</th>\n",
              "      <th>0.24</th>\n",
              "      <th>0.25</th>\n",
              "      <th>0.26</th>\n",
              "      <th>0.27</th>\n",
              "      <th>0.28</th>\n",
              "      <th>0.29</th>\n",
              "      <th>0.30</th>\n",
              "      <th>0.31</th>\n",
              "      <th>0.32.2</th>\n",
              "      <th>0.33</th>\n",
              "      <th>0.34</th>\n",
              "      <th>0.35</th>\n",
              "      <th>0.36</th>\n",
              "      <th>0.37</th>\n",
              "      <th>0.38</th>\n",
              "      <th>0.39</th>\n",
              "      <th>0.40</th>\n",
              "      <th>0.41</th>\n",
              "      <th>0.42</th>\n",
              "      <th>0.778</th>\n",
              "      <th>0.43</th>\n",
              "      <th>0.44</th>\n",
              "      <th>3.756</th>\n",
              "      <th>61</th>\n",
              "      <th>278</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.104576</td>\n",
              "      <td>0.212922</td>\n",
              "      <td>0.280578</td>\n",
              "      <td>0.065439</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>0.095922</td>\n",
              "      <td>0.114233</td>\n",
              "      <td>0.105317</td>\n",
              "      <td>0.090087</td>\n",
              "      <td>0.239465</td>\n",
              "      <td>0.059837</td>\n",
              "      <td>0.541680</td>\n",
              "      <td>0.093950</td>\n",
              "      <td>0.058639</td>\n",
              "      <td>0.049215</td>\n",
              "      <td>0.248833</td>\n",
              "      <td>0.142617</td>\n",
              "      <td>0.184504</td>\n",
              "      <td>1.662041</td>\n",
              "      <td>0.085596</td>\n",
              "      <td>0.809728</td>\n",
              "      <td>0.121228</td>\n",
              "      <td>0.101667</td>\n",
              "      <td>0.094289</td>\n",
              "      <td>0.549624</td>\n",
              "      <td>0.265441</td>\n",
              "      <td>0.767472</td>\n",
              "      <td>0.124872</td>\n",
              "      <td>0.098937</td>\n",
              "      <td>0.102874</td>\n",
              "      <td>0.064767</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.097250</td>\n",
              "      <td>0.047846</td>\n",
              "      <td>0.105435</td>\n",
              "      <td>0.097498</td>\n",
              "      <td>0.136983</td>\n",
              "      <td>0.013204</td>\n",
              "      <td>0.078646</td>\n",
              "      <td>0.064848</td>\n",
              "      <td>0.043676</td>\n",
              "      <td>0.132367</td>\n",
              "      <td>0.046109</td>\n",
              "      <td>0.079213</td>\n",
              "      <td>0.301289</td>\n",
              "      <td>0.179863</td>\n",
              "      <td>0.005446</td>\n",
              "      <td>0.031876</td>\n",
              "      <td>0.038583</td>\n",
              "      <td>0.139061</td>\n",
              "      <td>0.016980</td>\n",
              "      <td>0.268960</td>\n",
              "      <td>0.075827</td>\n",
              "      <td>0.044248</td>\n",
              "      <td>5.191827</td>\n",
              "      <td>52.170870</td>\n",
              "      <td>283.290435</td>\n",
              "      <td>0.393913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.305387</td>\n",
              "      <td>1.290700</td>\n",
              "      <td>0.504170</td>\n",
              "      <td>1.395303</td>\n",
              "      <td>0.672586</td>\n",
              "      <td>0.273850</td>\n",
              "      <td>0.391480</td>\n",
              "      <td>0.401112</td>\n",
              "      <td>0.278643</td>\n",
              "      <td>0.644816</td>\n",
              "      <td>0.201565</td>\n",
              "      <td>0.861791</td>\n",
              "      <td>0.301065</td>\n",
              "      <td>0.335219</td>\n",
              "      <td>0.258871</td>\n",
              "      <td>0.825881</td>\n",
              "      <td>0.444099</td>\n",
              "      <td>0.530930</td>\n",
              "      <td>1.775669</td>\n",
              "      <td>0.509821</td>\n",
              "      <td>1.200938</td>\n",
              "      <td>1.025866</td>\n",
              "      <td>0.350321</td>\n",
              "      <td>0.442681</td>\n",
              "      <td>1.671511</td>\n",
              "      <td>0.887043</td>\n",
              "      <td>3.367639</td>\n",
              "      <td>0.538631</td>\n",
              "      <td>0.593389</td>\n",
              "      <td>0.456729</td>\n",
              "      <td>0.403435</td>\n",
              "      <td>0.328594</td>\n",
              "      <td>0.555966</td>\n",
              "      <td>0.329480</td>\n",
              "      <td>0.532315</td>\n",
              "      <td>0.402664</td>\n",
              "      <td>0.423493</td>\n",
              "      <td>0.220675</td>\n",
              "      <td>0.434718</td>\n",
              "      <td>0.349953</td>\n",
              "      <td>0.361243</td>\n",
              "      <td>0.766900</td>\n",
              "      <td>0.223835</td>\n",
              "      <td>0.622042</td>\n",
              "      <td>1.011787</td>\n",
              "      <td>0.911214</td>\n",
              "      <td>0.076283</td>\n",
              "      <td>0.285765</td>\n",
              "      <td>0.243497</td>\n",
              "      <td>0.270377</td>\n",
              "      <td>0.109406</td>\n",
              "      <td>0.815726</td>\n",
              "      <td>0.245906</td>\n",
              "      <td>0.429388</td>\n",
              "      <td>31.732891</td>\n",
              "      <td>194.912453</td>\n",
              "      <td>606.413764</td>\n",
              "      <td>0.488669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.588000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.310000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.275500</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.382500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.640000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.314250</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.705250</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>265.250000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.540000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>42.810000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>7.270000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>5.260000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>2.610000</td>\n",
              "      <td>9.670000</td>\n",
              "      <td>5.550000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.410000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>17.100000</td>\n",
              "      <td>5.450000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>20.830000</td>\n",
              "      <td>16.660000</td>\n",
              "      <td>33.330000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.690000</td>\n",
              "      <td>6.890000</td>\n",
              "      <td>8.330000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>3.570000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.420000</td>\n",
              "      <td>22.050000</td>\n",
              "      <td>2.170000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.385000</td>\n",
              "      <td>9.752000</td>\n",
              "      <td>4.081000</td>\n",
              "      <td>32.478000</td>\n",
              "      <td>6.003000</td>\n",
              "      <td>19.829000</td>\n",
              "      <td>1102.500000</td>\n",
              "      <td>9989.000000</td>\n",
              "      <td>15841.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0         0.64  ...           278            1\n",
              "count  4600.000000  4600.000000  ...   4600.000000  4600.000000\n",
              "mean      0.104576     0.212922  ...    283.290435     0.393913\n",
              "std       0.305387     1.290700  ...    606.413764     0.488669\n",
              "min       0.000000     0.000000  ...      1.000000     0.000000\n",
              "25%       0.000000     0.000000  ...     35.000000     0.000000\n",
              "50%       0.000000     0.000000  ...     95.000000     0.000000\n",
              "75%       0.000000     0.000000  ...    265.250000     1.000000\n",
              "max       4.540000    14.280000  ...  15841.000000     1.000000\n",
              "\n",
              "[8 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkbrCX4-7S50",
        "outputId": "6e39866f-4e2f-4cf2-94ba-8c1d869cbec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Checking for Duplicates in the set\n",
        "spam.duplicated().sum()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "391"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVzQSnru7W-_"
      },
      "source": [
        "# dropping duplicate values \n",
        "spam.drop_duplicates(keep=False,inplace=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSrqWbvZ7bSn",
        "outputId": "c411ec12-12da-47df-d330-b2c7cb9e274d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Checking to confirm they have been removed\n",
        "spam.duplicated().sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLncoGtt7fvo",
        "outputId": "e24e7d1e-d5ea-47ea-f7a8-bb323517d302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Checking for missing values in the  set\n",
        "spam.isna().sum()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         0\n",
              "0.64      0\n",
              "0.64.1    0\n",
              "0.1       0\n",
              "0.32      0\n",
              "0.2       0\n",
              "0.3       0\n",
              "0.4       0\n",
              "0.5       0\n",
              "0.6       0\n",
              "0.7       0\n",
              "0.64.2    0\n",
              "0.8       0\n",
              "0.9       0\n",
              "0.10      0\n",
              "0.32.1    0\n",
              "0.11      0\n",
              "1.29      0\n",
              "1.93      0\n",
              "0.12      0\n",
              "0.96      0\n",
              "0.13      0\n",
              "0.14      0\n",
              "0.15      0\n",
              "0.16      0\n",
              "0.17      0\n",
              "0.18      0\n",
              "0.19      0\n",
              "0.20      0\n",
              "0.21      0\n",
              "0.22      0\n",
              "0.23      0\n",
              "0.24      0\n",
              "0.25      0\n",
              "0.26      0\n",
              "0.27      0\n",
              "0.28      0\n",
              "0.29      0\n",
              "0.30      0\n",
              "0.31      0\n",
              "0.32.2    0\n",
              "0.33      0\n",
              "0.34      0\n",
              "0.35      0\n",
              "0.36      0\n",
              "0.37      0\n",
              "0.38      0\n",
              "0.39      0\n",
              "0.40      0\n",
              "0.41      0\n",
              "0.42      0\n",
              "0.778     0\n",
              "0.43      0\n",
              "0.44      0\n",
              "3.756     0\n",
              "61        0\n",
              "278       0\n",
              "1         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng6RWgRk7k38"
      },
      "source": [
        "There are no missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXDR-l809EKw"
      },
      "source": [
        "Visualization and graphing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50075Odb9JFk",
        "outputId": "abd0b93d-d44c-4db3-df17-d7d3e3efb403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "## Countplot for the target variable Not Spam and Spam\n",
        "sns.countplot(spam[\"1\"])\n",
        "plt.title(\" Bar Chart of Not Spam and Spam\")\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmPy1QuV90eK"
      },
      "source": [
        "No spam are more compare to the spam messages "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbZR1sa29_es"
      },
      "source": [
        "##Building and Implementing the model##\n",
        "\n",
        "\n",
        "##**Naive Bayes classifier**##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHF4L5Ty-k3k"
      },
      "source": [
        "#Base Model gaussian \n",
        "\n",
        "\n",
        "#Working with A test Size of 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZd2brYj98ax"
      },
      "source": [
        "\"\"# Splitting the dataset x and y..\n",
        "X = spam.iloc[:,:57\n",
        "             ]\n",
        "# .values.reshape(-1,1)\n",
        "y = spam.iloc[:,-1]\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snCUqkbj-s58"
      },
      "source": [
        "# Test train\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlkCwTRq-0mB"
      },
      "source": [
        "# Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "mscaler = StandardScaler()\n",
        "mscaler.fit(X_train)\n",
        "\n",
        "X_train = mscaler.transform(X_train)\n",
        "X_test = mscaler.transform(X_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm2MIA3w-43b"
      },
      "source": [
        "# Importing\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTygXsud-8ms",
        "outputId": "527dcd8a-e34b-4644-ed3c-6b0aab34750d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Trainin our model\n",
        "model = GaussianNB()  \n",
        "model.fit(X_train, y_train) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvoBafyk_AOq"
      },
      "source": [
        "# prediction\n",
        "predicted = model.predict(X_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ics6WpxE_E9j",
        "outputId": "befd127c-19d3-416a-97d6-753f146d6899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Accuracy\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, predicted))\n",
        "print(classification_report(y_test, predicted))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[340 136]\n",
            " [ 11 319]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.71      0.82       476\n",
            "           1       0.70      0.97      0.81       330\n",
            "\n",
            "    accuracy                           0.82       806\n",
            "   macro avg       0.83      0.84      0.82       806\n",
            "weighted avg       0.86      0.82      0.82       806\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyC01jFe_KTg"
      },
      "source": [
        "Gaussian recored an accuracy of 82%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye--HzFIMmjT"
      },
      "source": [
        "##Gaussian performing at 0.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QrMiymLMjnu"
      },
      "source": [
        "# Test train\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orAkFL7vM0m4"
      },
      "source": [
        "# Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "mscaler = StandardScaler()\n",
        "mscaler.fit(X_train)\n",
        "\n",
        "X_train = mscaler.transform(X_train)\n",
        "X_test = mscaler.transform(X_test)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lStHXRKLM2bM"
      },
      "source": [
        "# Importing\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTv6Jqd4M-26",
        "outputId": "640c001c-0194-40fa-8320-f477b7787ed3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Trainin our model\n",
        "model = GaussianNB()  \n",
        "model.fit(X_train, y_train) "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htY0er0zNDAS"
      },
      "source": [
        "# prediction\n",
        "predicted = model.predict(X_test)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-Qhn2B_NHDg",
        "outputId": "30b978d6-36ba-4920-cc09-b129c6c74e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Accuracy\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, predicted))\n",
        "print(classification_report(y_test, predicted))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[340 136]\n",
            " [ 11 319]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.71      0.82       476\n",
            "           1       0.70      0.97      0.81       330\n",
            "\n",
            "    accuracy                           0.82       806\n",
            "   macro avg       0.83      0.84      0.82       806\n",
            "weighted avg       0.86      0.82      0.82       806\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kljcycgNTl-"
      },
      "source": [
        "The accuracy of the Gaussian Naive bayes has decreased from 82% to 75%when the test data was split 0.7/0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyXw8wk-_TK0"
      },
      "source": [
        "#Base Model \n",
        "\n",
        "##Bernoulli Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOG6zpsj_NRQ"
      },
      "source": [
        "# Import\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nm-F-RK_5GZ"
      },
      "source": [
        "# train test...\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lihq7FdNAIH3"
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFeoVLec_9lV",
        "outputId": "81d7e390-116b-4172-e155-0d768b660889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        " # Creating oour Bernoulli Naive Bayes object with prior probabilities of each class\n",
        "model = BernoulliNB()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# model score\n",
        "model.score(X_train, y_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8879925535215638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av4wy5jQAOXt"
      },
      "source": [
        "# Prediction\n",
        "ypredi = model.predict(X_test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONgvWn30AUYY",
        "outputId": "01b0852f-d652-4bf7-b0cc-884da1b28e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Accuracy Summary\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, ypredi))\n",
        "print(classification_report(y_test, ypredi))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[472  37]\n",
            " [ 49 248]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92       509\n",
            "           1       0.87      0.84      0.85       297\n",
            "\n",
            "    accuracy                           0.89       806\n",
            "   macro avg       0.89      0.88      0.88       806\n",
            "weighted avg       0.89      0.89      0.89       806\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_BFVQtJAekJ"
      },
      "source": [
        "Bernoulli Naive Bayes accuracy is 89% at 0.8/0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpDWtE6kBDAh"
      },
      "source": [
        "# Working with A test Size of 0.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqracyI2Aq4A"
      },
      "source": [
        "# train test...\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6) "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26KVAF48BL_S",
        "outputId": "b4a79953-0574-4098-9210-367b6010b1a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Creating oour Bernoulli Naive Bayes object with prior probabilities of each class\n",
        "model = BernoulliNB()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JTuDoBKBMhP",
        "outputId": "26043dc1-83d2-4f82-b3b7-a5f82ce467db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# model score\n",
        "model.score(X_train, y_train)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8900709219858156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTDLcFM-BaWe"
      },
      "source": [
        "# Predict\n",
        "ypredb = model.predict(X_test)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojx2XfBaBfep",
        "outputId": "48b73901-2d23-46ba-f37d-e54f450f4e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Accuracy Summary\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, ypredb))\n",
        "print(classification_report(y_test, ypredb))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[696  50]\n",
            " [ 84 379]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91       746\n",
            "           1       0.88      0.82      0.85       463\n",
            "\n",
            "    accuracy                           0.89      1209\n",
            "   macro avg       0.89      0.88      0.88      1209\n",
            "weighted avg       0.89      0.89      0.89      1209\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrmDjargBjc3"
      },
      "source": [
        "### with sample size of 0.3 Bernoulli Naive Bayes recorded an accuracy of 89%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J09NUxUxCNma"
      },
      "source": [
        "# Working with A test Size of 0.4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfuCVA5bBt2N"
      },
      "source": [
        "# train test...\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6) "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rcUD3ZqCaEd",
        "outputId": "0035ced4-27e2-4a6e-8cba-c96723e515e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Creating oour Bernoulli Naive Bayes object with prior probabilities of each class\n",
        "model = BernoulliNB()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP08wL7pCe-R",
        "outputId": "10aaa56e-68c2-47ee-d54a-5c7a220f8fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# model score\n",
        "model.score(X_train, y_train)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8911874224244932"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfwtm_4eCqJu"
      },
      "source": [
        "# Predict\n",
        "ypredb = model.predict(X_test)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSFUobs_CuW9",
        "outputId": "bbcd3acd-5af7-4035-d849-03a9352465b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Accuracy Summary\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, ypredb))\n",
        "print(classification_report(y_test, ypredb))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[895  67]\n",
            " [111 539]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91       962\n",
            "           1       0.89      0.83      0.86       650\n",
            "\n",
            "    accuracy                           0.89      1612\n",
            "   macro avg       0.89      0.88      0.88      1612\n",
            "weighted avg       0.89      0.89      0.89      1612\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOThsZoTC13j"
      },
      "source": [
        "### with sample size of 0.4 Bernoulli Naive Bayes recorded an accuracy of 89%\n",
        "\n",
        "It can be noted that using this dataset Bernoulli Naive Bayes is not affecting by the 3 testing split of 80/20, 70/30 and 60/40.\n",
        "The accuracy remains the same at 89% \n",
        "\n",
        "We can conclude that  Bernoulli Naive Bayes performed better than Gaussian Naives Bayes on this dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBn7vRdODpFc"
      },
      "source": [
        "## **Challenging the solution**\n",
        "\n",
        "\n",
        "My thinking is, is there anyway the accuarcy of prediction can be improved \n",
        "Am going to use XGBoost "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JYsakhdIJfw"
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCxaBidvC6eG",
        "outputId": "8d267214-a652-452b-9bf2-8f4806e23661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "#predicting with xgboost\n",
        "Model_main = XGBClassifier(n_estimators = 100, learning_rate = 0.25)\n",
        "## Fitting our model in the x and y variables\n",
        "Model_main.fit(X_test, y_test)\n",
        "\n",
        "# Calculating Feature Importance\n",
        "\n",
        "print(Model_main.feature_importances_)\n",
        "importance = pd.DataFrame({'feature':X.columns,'importance':np.round(Model_main.feature_importances_,3)})\n",
        "importance = importance.sort_values('importance',ascending=False).set_index('feature')\n",
        "ten = importance.head(10)\n",
        "\n",
        "\n",
        "# make predictions for test data\n",
        "\n",
        "y_pred = Model_main.predict(X_test)\n",
        "predicting = [round(value) for value in y_pred]\n",
        "# Model Accuracy\n",
        "print('Accuracy is :', Model_main.score(X_test, y_test) * 100)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00514532 0.00585092 0.00368546 0.         0.01573291 0.00655374\n",
            " 0.14302193 0.01888869 0.00247577 0.00509941 0.01298688 0.00616409\n",
            " 0.00334281 0.         0.         0.04660176 0.0089702  0.0070917\n",
            " 0.00454546 0.00501326 0.02930333 0.0075167  0.01700525 0.03339931\n",
            " 0.05342229 0.00891117 0.03224153 0.03157353 0.00497793 0.\n",
            " 0.         0.         0.00380214 0.         0.00647577 0.00501179\n",
            " 0.03393682 0.         0.00385356 0.         0.         0.01755236\n",
            " 0.02182646 0.00451287 0.01262153 0.03722156 0.         0.00610236\n",
            " 0.00897743 0.00383843 0.         0.07726138 0.16047406 0.00478847\n",
            " 0.02511021 0.02395303 0.02315839]\n",
            "Accuracy is : 97.64267990074443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1IOaJJSPZvx"
      },
      "source": [
        "The outcome indicates that if we had implement xgboostclassifier on dataset it would have performed better than both the Guassian Naive Bayes and  Bernoulli Naive Bayes with an accuracy of 97.64%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2l0k-psIYkx",
        "outputId": "c3317003-8a52-4b59-d4e2-3bac6ab371c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# Feature Importance Visualization\n",
        "sns.set(style = 'whitegrid', context = 'notebook')\n",
        "ten.plot.bar()\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdiT41NkIcPf"
      },
      "source": [
        "The idea is to see which features are important in this dataset, the level of importancy increases "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54QlByj6JTam"
      },
      "source": [
        "##I would like to implement the xgboost model to see if there are changes to the accuracy  on the important features only##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr6wVIFeItwn"
      },
      "source": [
        "# the x values are for the important features only \n",
        "X = spam[[\"0.43\",\"0.3\",\"0.778\",\"0.16\",\"0.32\",\"0.37\",\"0.28\",\"0.15\",\"0.19\",\"0.18\"]].values\n",
        "y = spam[\"1\"].values"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwZTsHBZJ-7p"
      },
      "source": [
        "# train test...\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6) "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YegSMgnKEjg"
      },
      "source": [
        "from xgboost import plot_importance\n",
        "from xgboost.sklearn import XGBClassifier"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZf7TnGFKJK3",
        "outputId": "bbdfd37a-1be1-476c-fea8-0b180582baac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Creating Bernoulli Naive Bayes object with prior probabilities of each class\n",
        "model = BernoulliNB()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g46gvI1vKQfD",
        "outputId": "8eda675b-fe8e-4b9b-ce28-c9a20974b4e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# model score\n",
        "model.score(X_train, y_train)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8986346710798511"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCkIcVgsKUSx"
      },
      "source": [
        "# Predict\n",
        "ypredc = model.predict(X_test)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tFq5s-SKYli",
        "outputId": "c564a422-0ba2-463c-bfb5-5cafa28f40d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Accuracy imports\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, ypredc))\n",
        "print(classification_report(y_test, ypredc))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[924  38]\n",
            " [122 528]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92       962\n",
            "           1       0.93      0.81      0.87       650\n",
            "\n",
            "    accuracy                           0.90      1612\n",
            "   macro avg       0.91      0.89      0.89      1612\n",
            "weighted avg       0.90      0.90      0.90      1612\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYscfnVPKwIO"
      },
      "source": [
        " Bernoulli model got an accuracy of 90% at 0.4, after using xgboost compared to get important features to 89% when xgboost was not used "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnNk3TkJRY_G"
      },
      "source": [
        "### Gussian recored an accuracy of about an average of 82% compared to Bernolli which scored about 89%, seems that the bernouli classification  performed better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y40TTL3RkUly"
      },
      "source": [
        "##Overview##\n",
        "\n",
        "Data exploration was done , duplicated values where removed, missing values were replaced with mode of the data in they respective columns\n",
        "\n",
        "Outliers and data distribution was also addressed \n",
        "\n",
        "\n",
        "\n",
        "##Modelling##\n",
        "\n",
        "I used based model and performed feature importance \n",
        "\n",
        "\n",
        "##Model Tunning ##\n",
        "\n",
        "cross validation was used to i dentified the best model \n",
        "\n",
        "\n",
        "##**Observations**##\n",
        "\n",
        "**Titanic dataset**\n",
        "\n",
        "There was outliers in the fare, this is due to different classes in the flight so these outliers were not dropped \n",
        "\n",
        "There were more men who survive the tragedy than femals \n",
        "\n",
        "In the dataset there was higher number of people who didn't survive who were age 20 to 30 years, this could be attributed to the fact that this age bracket were the majority in the Tatanic ship\n",
        "\n",
        "##**Cross Validation**##\n",
        "\n",
        "10-fold cross validation tells us that K=7 results in the lowest validation error.\n",
        "\n",
        "After parameter tuning The accuracy score came to 62% Out of 179 people, 111 where predicted correctly\n",
        "\n",
        "\n",
        "##**Naïve Classifier**##\n",
        "\n",
        "spambase Dataset\n",
        "\n",
        "About 391 duplicates where identified There were no missing values\n",
        "\n",
        "Modeling\n",
        "\n",
        "A base model for Gaussian resulted in an accuracy score of 82% at 80/20\n",
        "A base model for Bernoulli resulted in an accuracy score of 89%\n",
        "\n",
        "Show that Bernoulli was better Gaussian recorded an accuracy of about an average of 82% compared to Bernoulli which scored about 89%, seems that the\n",
        "\n",
        "Bernoulli classification performed better, predicted as sparm or not spam\n",
        "\n",
        "After challenging the solution\n",
        "\n",
        "I used xgboost and the prediction results were 90%  indicating that the model can be improved by applying xgboost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OJjFjvcohj6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}